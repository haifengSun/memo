{"content":[{"key":"header","data":{"content":"【论文笔记】Advances and Open Problems in Federated Learning","level":2},"common":{"quote":false}},{"key":"paragraph","data":{"content":"联邦学习的综述文章，介绍联邦学习的过去和现在，并思考未来"},"common":{}},{"key":"header","data":{"content":"Introduction","level":3},"common":{}},{"key":"list","data":{"style":"unordered","items":[{"content":"联邦学习不是一个简单的问题，需要各个领域和各种专家合作，这篇文章主要就是点出可能可以在哪里发力","style":"unordered","closed":false,"items":[]},{"content":"永远绕不开的 Google Gboard 的例子","style":"unordered","closed":false,"items":[]},{"content":"两个关键设定：","style":"unordered","closed":false,"items":[{"content":"数据在本地产生，并且不会集中到一起；","style":"unordered","closed":false,"items":[]},{"content":"训练步骤则需要一个中心服务器，但该服务器无法获取原始数据","style":"unordered","closed":false,"items":[]}]}]},"common":{"quote":false}},{"key":"image","data":{"image":{"url":"https://cdn.jsdelivr.net/gh/haifengSun/memo/geekeditor_images/2022-9-22/1663833561404-image.png","name":"image.png"},"caption":"横向联邦.png","withCaption":true},"common":{}},{"key":"list","data":{"style":"unordered","items":[{"content":"联邦学习模型的生命周期","style":"unordered","closed":false,"items":[{"content":"问题定义","style":"unordered","closed":false,"items":[]},{"content":"客户端工具","style":"unordered","closed":false,"items":[]},{"content":"仿真原型（可选）","style":"unordered","closed":false,"items":[]},{"content":"联邦模型训练","style":"unordered","closed":false,"items":[]},{"content":"联邦模型评估","style":"unordered","closed":false,"items":[]},{"content":"模型部署","style":"unordered","closed":false,"items":[]}]},{"content":"典型的联邦训练过程","style":"unordered","closed":false,"items":[{"content":"客户端选择","style":"unordered","closed":false,"items":[]},{"content":"数据广播（包括模型参数和计算图等）","style":"unordered","closed":false,"items":[]},{"content":"客户端计算","style":"unordered","closed":false,"items":[]},{"content":"数据聚合","style":"unordered","closed":false,"items":[]},{"content":"模型更新","style":"unordered","closed":false,"items":[]}]}]},"common":{"quote":false}},{"key":"header","data":{"content":"Relaxing the Core FL Assumptions","level":3},"common":{"quote":false}},{"key":"list","data":{"style":"unordered","items":[{"content":"完全去中心化/点对点分布式学习","style":"unordered","closed":false,"items":[{"content":"可靠的中央服务器不一定存在，并且在客户端非常多的时候，服务器可能成为<strong>瓶颈</strong>","style":"unordered","closed":false,"items":[]},{"content":"完全去中心化学习的核心思路是通过客户端间的点对点通信取代与服务器通信，但<strong>谁来做决策</strong>是一个问题，相关的决策点有：","style":"unordered","closed":false,"items":[{"content":"训练什么模型","style":"unordered","closed":false,"items":[]},{"content":"使用什么参数和超参数","style":"unordered","closed":false,"items":[]},{"content":"谁负责调试和评估","style":"unordered","closed":false,"items":[]}]},{"content":"算法挑战：离真正在现实世界中可用还有很多难题要解决（注：做科研科研，目前距离商业落地还有很长的路要走，具体而言在于谁为服务买单，ToB？or ToC？目前落地做的最好的行业是<strong>医药</strong>和<strong>金融</strong>）","style":"unordered","closed":false,"items":[{"content":"网络拓扑结构和异步计算对去中心化 SGD 的影响：需要在客户端和网络都有较大不确定性的情况下保证<strong>鲁棒性</strong>","style":"unordered","closed":false,"items":[]},{"content":"去中心化 SGD 的本地更新：如何能够在<strong>非 IID 数据</strong>分布的条件下让模型收敛，以及如何设计最快收敛的模型平均策略","style":"unordered","closed":false,"items":[]},{"content":"个性化和信任机制：如何从大量个人化的模型（本地训练）中学习到一个好的大模型，另一个角度是如何在大模型的基础上让每个人都有定制化的模型。需要解决鲁棒性和恶意攻击者的问题，这就涉及到激励机制的问题。","style":"unordered","closed":false,"items":[]},{"content":"梯度压缩和量化方法：客户端一般算力和电量都不多，所以需要对带宽和计算量进行压缩","style":"unordered","closed":false,"items":[]}]},{"content":"实践挑战：如何在现实世界中落地呢？","style":"unordered","closed":false,"items":[{"content":"需要防止客户端 A 利用全局模型来还原客户端 B 原始数据这个操作（注：即进行隐私保护），一般来说会用差分隐私来解决。（注：现实世界的挑战主要是基于恶意攻击者，学术研究中一般都假设是半诚实）","style":"unordered","closed":false,"items":[]}]}]},{"content":"跨数据孤岛的联邦学习","style":"unordered","closed":false,"items":[{"content":"数据分割：对应横向联邦、纵向联邦、联邦迁移学习，以及对应的常用算法","style":"unordered","closed":false,"items":[]},{"content":"激励机制：参与者可能是竞争对手，如何来分配收益？","style":"unordered","closed":false,"items":[]},{"content":"差分隐私：目前该领域没有系统性探索","style":"unordered","closed":false,"items":[]},{"content":"张量分解：这个领域感觉研究也不算多","style":"unordered","closed":false,"items":[]}]}]},"common":{"quote":false}},{"key":"image","data":{"image":{"url":"https://cdn.jsdelivr.net/gh/haifengSun/memo/geekeditor_images/2022-9-22/1663834073504-image.png","name":"image.png"},"caption":"image.png","withCaption":true},"common":{}},{"key":"list","data":{"style":"unordered","items":[{"content":"拆分学习：在客户端和服务器之间按层划分模型，这样客户端的数据服务器是无法感知的，另一种模式下（上图 b），连标签都不需要共享。不过目前这个领域还比较新","style":"unordered","closed":false,"items":[]}]},"common":{"quote":false}},{"key":"header","data":{"content":"Improving Efficiency and Effectiveness","level":3},"common":{"quote":false}},{"key":"list","data":{"style":"unordered","items":[{"content":"基本挑战是<strong>非独立同分布(non-IID)数据</strong>的存在","style":"unordered","closed":false,"items":[{"content":"非同分布的客户端分布：即不同客户端中的样本产生的机制可能有差别（比如地域、国家等）","style":"unordered","closed":false,"items":[{"content":"特征分布倾斜（协变量漂移）：比如手写识别中，同一个字不同人写法不一样","style":"unordered","closed":false,"items":[]},{"content":"标签分布倾斜（先验概率漂移）：比如不同地区的人用的表情不一样 or 熊猫在中国更多","style":"unordered","closed":false,"items":[]},{"content":"标签相同，特征不同（概念漂移）：不同国家的品牌差别就很大","style":"unordered","closed":false,"items":[]},{"content":"特征相同，标签不同（概念漂移）：不同人对“呵呵”的情绪评价不一样","style":"unordered","closed":false,"items":[]},{"content":"数量倾斜或者不平衡","style":"unordered","closed":false,"items":[]}]},{"content":"现实中各种情况都有可能，目前最受关注的是标签分布倾斜","style":"unordered","closed":false,"items":[]},{"content":"对于不同的 non-IID 分布可能需要制定不同的策略","style":"unordered","closed":false,"items":[]},{"content":"违反独立性：在训练过程中可能因为概率分布变化而导致其违反独立性","style":"unordered","closed":false,"items":[]},{"content":"数据集漂移：即训练集和测试集的分布不同","style":"unordered","closed":false,"items":[]},{"content":"处理 non-IID 数据的策略","style":"unordered","closed":false,"items":[{"content":"可能可以增加数据使得不同客户端的数据更加相似，比如可以是全局共享的小数据集，或一个公开数据集，或原始数据的蒸馏结果","style":"unordered","closed":false,"items":[]},{"content":"如何构建目标函数变得更加重要","style":"unordered","closed":false,"items":[]},{"content":"每个设备上的本地数据都可以进行训练，相当于都有定制模型，只需要处理好集成的问题（注：我觉得这个是一个重要的思路）","style":"unordered","closed":false,"items":[]}]}]},{"content":"联邦学习的优化算法","style":"unordered","closed":false,"items":[{"content":"特别关注 non-IID 且不平衡的数据、有限的带宽、不可靠和有限的可用设备","style":"unordered","closed":false,"items":[]},{"content":"需要考虑与其他技术的可组合性","style":"unordered","closed":false,"items":[]},{"content":"最常见的优化算法是联邦平均算法","style":"unordered","closed":false,"items":[]},{"content":"注：这部分比较偏向理论研究，等待学术界和大公司突破了","style":"unordered","closed":false,"items":[]}]},{"content":"多任务学习，个性化和元学习","style":"unordered","closed":false,"items":[{"content":"如果将每个客户端的本地数据集学习视为一项单独的任务，那么实际上这个就是分布式的多任务学习","style":"unordered","closed":false,"items":[]},{"content":"引入用于多任务联合学习的 MOCHA 算法直接解决了通信效率、掉队者和容错的挑战","style":"unordered","closed":false,"items":[]},{"content":"何时进行全局联邦学习最好","style":"unordered","closed":false,"items":[]}]},{"content":"适用于联邦学习的机器学习工作流（注：这一部分在应用中尤为重要，因为去中心化的设定使得原来的标准化流水线不再适用）","style":"unordered","closed":false,"items":[{"content":"超参数调整：不可能大量尝试，如何快速找到合适的超参（注：这个主要针对深度学习，传统机器学习没有那么多超参）","style":"unordered","closed":false,"items":[]},{"content":"神经结构设计：因为数据对于设计模型的人不可见，所以更需要 NAS 技术来找到最佳结果（注：传统机器学习相对来说这个问题不明显）","style":"unordered","closed":false,"items":[]},{"content":"调试和可解释性：目前还需要有很多突破","style":"unordered","closed":false,"items":[]}]},{"content":"通信和压缩：可能是主要瓶颈，目前的研究还比较难落地，比较理想化","style":"unordered","closed":false,"items":[{"content":"压缩目标：就是要压缩啥","style":"unordered","closed":false,"items":[{"content":"梯度压缩","style":"unordered","closed":false,"items":[]},{"content":"模型广播压缩","style":"unordered","closed":false,"items":[]},{"content":"减少本地计算","style":"unordered","closed":false,"items":[]}]},{"content":"差分隐私和安全聚合的兼容：现有的噪声添加机制和用于减少通信的标准量化方法不兼容","style":"unordered","closed":false,"items":[]}]},{"content":"应用到更多类型的机器学习问题和模型","style":"unordered","closed":false,"items":[{"content":"从监督学习到强化学习、半监督学习、无监督学习、主动学习和在线学习（注：能把有监督落地就很难了，其他的停留在论文中的可能性较大）","style":"unordered","closed":false,"items":[]}]}]},"common":{"quote":false}},{"key":"paragraph","data":{"content":""},"common":{}},{"key":"paragraph","data":{"content":""},"common":{}}],"typeset":"basic","layout":"print","basic":{"title":"","cover":"","summary":""}}